{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxji8SyBNQzZP3b0NicnlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuyeoungJun/graduation-project/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0IujeZmm1d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9oCeHOSn1N2",
        "cellView": "code"
      },
      "source": [
        "\n",
        "def cleaning(text):\n",
        "    result = []\n",
        "    hangle = re.compile('[^ A-Za-zㄱ-ㅣ가-힣]+')\n",
        "    for word in text.review:\n",
        "        if type(word) == int:\n",
        "            continue\n",
        "        result.append(hangle.sub('', word))\n",
        "        # result = hangle.findall(word)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    # 불용어 지정\n",
        "    # stop_words = ['저','.','!!'.'ㅎㅎ','ㅋㅋ','~','....','으로','로']\n",
        "\n",
        "    okt = Okt()\n",
        "    hannanum = Hannanum()\n",
        "    kkma = Kkma()\n",
        "\n",
        "    result = []\n",
        "    dic = {}\n",
        "\n",
        "    # log_point = text[3556:]\n",
        "\n",
        "    log_count = 0\n",
        "\n",
        "    for word in text.review:\n",
        "        log_count+=1\n",
        "        temp = []\n",
        "        if word == ' ':\n",
        "          temp.append(0)\n",
        "          temp.append(0)\n",
        "          temp.append(0)\n",
        "          temp.append(0)\n",
        "          temp.append(0)\n",
        "          temp.append(0)\n",
        "          temp.append(0)\n",
        "          result.append(temp)\n",
        "          continue\n",
        "          \n",
        "        print(\"log \", log_count, \" complete \")\n",
        "        \n",
        "        # word = cleaning()\n",
        "        temp.append(len(word))  # 1.길이\n",
        "        temp.append(len(kkma.nouns(word)))  # 2.명사 종류의 갯수\n",
        "        cnt_str = hannanum.pos(word)\n",
        "        v_count = 0\n",
        "        n_count = 0\n",
        "        p_count = 0\n",
        "\n",
        "        cnt_np = kkma.pos(word)\n",
        "        np_count = 0\n",
        "        ec_count = 0\n",
        "        ignore_count = 0\n",
        "        # print(word)\n",
        "        for cnt in cnt_np:\n",
        "            if (cnt[1] == \"NP\"):\n",
        "                np_count += 1  # 5.대명사\n",
        "            elif (cnt[1] == \"EC\" or cnt[1] == \"ECD\" or cnt[1] == \"ECE\" or cnt[1] == \"ECS\"):\n",
        "                ec_count += 1  # 6.접속사\n",
        "            elif (cnt[1] == \"VV\" or cnt[1] == \"VXV\"):\n",
        "                v_count += 1  # 3.동사\n",
        "            elif (cnt[1] == \"VA\" or cnt[1] == \"VXA\" or cnt[1] == \"XR\"):\n",
        "                p_count += 1  # 4.형용사\n",
        "            elif (cnt[0] == \"리뷰\" or cnt[0] == \"사진\"):\n",
        "                ignore_count += 1  # 7.리뷰관련 단어\n",
        "        temp.append(v_count)\n",
        "        temp.append(p_count)\n",
        "        temp.append(np_count)\n",
        "        temp.append(ec_count)\n",
        "        temp.append(ignore_count)\n",
        "        result.append(temp)\n",
        "        ## 희소한 단어: DIC 사용\n",
        "        dic_plus = kkma.pos(word)\n",
        "        for t in dic_plus:\n",
        "            if t[0] in dic:\n",
        "                dic[t[0]] += 1\n",
        "            else:\n",
        "                dic[t[0]] = 1\n",
        "\n",
        "    # 희귀 단어 count\n",
        "    print(len(text))\n",
        "\n",
        "    for i in range(len(text)):\n",
        "        #print(text.review[i])\n",
        "        if text.review[i] == ' ':\n",
        "            result[i].append(0)\n",
        "            continue\n",
        "        check = kkma.pos(text.review[i])\n",
        "        unique = 0\n",
        "        for t in check:\n",
        "            if t not in dic:\n",
        "                continue\n",
        "            if dic[t[0]] <= 3:\n",
        "                unique += 1\n",
        "        result[i].append(unique)  # 8.희귀단어\n",
        "\n",
        "    # 정규화\n",
        "    trans = np.transpose(result)\n",
        "\n",
        "    column_max = []\n",
        "    column_min = []\n",
        "    column_mean = []\n",
        "    column_std = []\n",
        "    for t in trans:\n",
        "        column_min.append(min(t))\n",
        "        column_max.append(max(t))\n",
        "        column_mean.append(np.mean(t))\n",
        "        column_std.append(np.std(t))\n",
        "    for i in range(len(result)):\n",
        "        for j in range(len(result[i])):\n",
        "            result[i][j] = (result[i][j] - column_min[j]) / (column_max[j] - column_min[j]+0.00001)\n",
        "\n",
        "    df = pd.DataFrame(result)\n",
        "    print(\"store check\")\n",
        "    df.to_csv('/content/drive/My Drive/graduation_project/input/preprocess1.csv', mode='w')\n",
        "    df.fillna(0)\n",
        "    return df.fillna(0)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    text = pd.read_csv('/content/drive/My Drive/graduation_project/input/labeling_data.csv', engine='python',\n",
        "                       encoding='utf-8', names=['review', 'label'])\n",
        "    #clean = cleaning(text[2500:].fillna(1))\n",
        "    #clean = pd.DataFrame(clean, columns=[\"review\"])\n",
        "    #x = preprocessing(clean)\n",
        "    #learning(x, text.label.fillna(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3TwZgHb4AUr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Okt\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "from torchvision import transforms\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8KkaoE4vEPq"
      },
      "source": [
        "class Dataset(data_utils.Dataset):\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'X': self.X[idx], 'y': self.y[idx]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLPRegressor, self).__init__()\n",
        "        h1 = nn.Linear(len(X_features), 50)\n",
        "        h2 = nn.Linear(50, 100)\n",
        "        h3 = nn.Linear(100, 100)\n",
        "        h4 = nn.Linear(100, 35)\n",
        "        h5 = nn.Linear(35, 1)\n",
        "        h6 = nn.Linear(100,1024)\n",
        "        h7 = nn.Linear(1024,1024)\n",
        "        h8 = nn.Linear(1024,100)\n",
        "\n",
        "        self.hidden = nn.Sequential(\n",
        "            h1,\n",
        "            nn.Tanh(),\n",
        "            h2,\n",
        "            nn.Tanh(),\n",
        "            h3,\n",
        "            nn.Tanh(),\n",
        "            h6,\n",
        "            nn.Tanh(),\n",
        "            h7,\n",
        "            nn.Tanh(),\n",
        "            h8,\n",
        "            nn.Tanh(),\n",
        "            h4,\n",
        "            nn.Tanh(),\n",
        "            h5\n",
        "            \n",
        "        )\n",
        "        if use_cuda:\n",
        "            self.hidden = self.hidden.cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.hidden(x)\n",
        "\n",
        "        return o\n",
        "\n",
        "trn = pd.read_csv(\"/content/drive/My Drive/graduation_project/input/train.csv\")\n",
        "val = pd.read_csv(\"/content/drive/My Drive/graduation_project/input/vali.csv\")\n",
        "X_features = [\"feature_1\",\"feature_2\",\"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\",\"feature_7\",\"feature_8\"]\n",
        "y_feature = [\"y\"]\n",
        "\n",
        "trn_X_pd, trn_y_pd = trn[X_features], trn[y_feature]\n",
        "val_X_pd, val_y_pd = val[X_features], val[y_feature]\n",
        "\n",
        "trn_X = torch.from_numpy(trn_X_pd.astype(float).values).float()\n",
        "trn_y = torch.from_numpy(trn_y_pd.astype(float).values).float()\n",
        "\n",
        "val_X = torch.from_numpy(val_X_pd.astype(float).values).float()\n",
        "val_y = torch.from_numpy(val_y_pd.astype(float).values).float()\n",
        "\n",
        "\n",
        "batch_size=128\n",
        "\n",
        "\n",
        "trn = Dataset(trn_X, trn_y)\n",
        "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val = Dataset(val_X, val_y)\n",
        "val_loader = data_utils.DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "tmp = next(iter(trn_loader))\n",
        "num_batches = len(trn_loader)\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "model = MLPRegressor()\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 1e-6\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 5000\n",
        "num_batches = len(trn_loader)\n",
        "\n",
        "trn_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    trn_loss_summary = 0.0\n",
        "    for i, trn in enumerate(trn_loader):\n",
        "        trn_X, trn_y = trn['X'], trn['y']\n",
        "        if use_cuda:\n",
        "            trn_X, trn_y = trn_X.cuda(), trn_y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        trn_pred = model(trn_X)\n",
        "        trn_loss = criterion(trn_pred, trn_y)\n",
        "        trn_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        trn_loss_summary += trn_loss\n",
        "\n",
        "        if (i + 1) % 15 == 0:\n",
        "            with torch.no_grad():\n",
        "                val_loss_summary = 0.0\n",
        "                for j, val in enumerate(val_loader):\n",
        "                    val_X, val_y = val['X'], val['y']\n",
        "                    if use_cuda:\n",
        "                        val_X, val_y = val_X.cuda(), val_y.cuda()\n",
        "                    val_pred = model(val_X)\n",
        "                    val_loss = criterion(val_pred, val_y)\n",
        "                    val_loss_summary += val_loss\n",
        "\n",
        "            print(\"epoch: {}/{} | step: {}/{} | trn_loss: {:.4f} | val_loss: {:.4f}\".format(\n",
        "                epoch + 1, num_epochs, i + 1, num_batches, (trn_loss_summary / 15) ** (1 / 2),\n",
        "                (val_loss_summary / len(val_loader)) ** (1 / 2)\n",
        "            ))\n",
        "\n",
        "            trn_loss_list.append((trn_loss_summary / 15) ** (1 / 2))\n",
        "            val_loss_list.append((val_loss_summary / len(val_loader)) ** (1 / 2))\n",
        "            trn_loss_summval = 0.0\n",
        "\n",
        "print(\"finish valing\")\n",
        "plt.figure(figsize=(16,9))\n",
        "x_range = range(len(trn_loss_list))\n",
        "plt.plot(x_range, trn_loss_list, label=\"trn\")\n",
        "plt.plot(x_range, val_loss_list, label=\"val\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"training steps\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "\n",
        "##TEST\n",
        "test = pd.read_csv(\"/content/drive/My Drive/graduation_project/input/test.csv\")\n",
        "\n",
        "test_X_pd, test_y_pd = test[X_features], test[y_feature]\n",
        "\n",
        "\n",
        "test_X = torch.from_numpy(test_X_pd.astype(float).values).float()\n",
        "test_y = torch.from_numpy(test_y_pd.astype(float).values).float()\n",
        "\n",
        "test = Dataset(test_X, test_y)\n",
        "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "for i, test in enumerate(test_loader):\n",
        "    test_X, test_y = test['X'], test['y']\n",
        "    if use_cuda:\n",
        "        test_X, test_y = test_X.cuda(), test_y.cuda()\n",
        "    output = model(test_X)\n",
        "    #print(\"check\",output)\n",
        "    _, prediction = torch.max(output,1)\n",
        "    #print(prediction)\n",
        "    pred = torch.round(output)\n",
        "    #print(pred)\n",
        "    correct += pred.eq(test_y).sum()\n",
        "\n",
        "print('Test set: Accuracy: {:.2f}%'.format(100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkTWBVdq9ATi"
      },
      "source": [
        "!pip install konlpy\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}